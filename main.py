from fastapi import FastAPI
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional
import logging
import logging.handlers
import sys
import os
import json
import re
import traceback
import torch
import datetime
from typing import Dict, Any
from fastapi import Body
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import login
from dotenv import load_dotenv

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤
LOG_DIR = 'logs'
os.makedirs(LOG_DIR, exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def setup_logger():
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.handlers.RotatingFileHandler(
                os.path.join(LOG_DIR, 'app_debug.log'),
                maxBytes=10*1024*1024,  # 10 –ú–ë
                backupCount=5,
                encoding='utf-8'
            ),
            logging.StreamHandler(sys.stdout)
        ]
    )

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–æ–≤ –±–∏–±–ª–∏–æ—Ç–µ–∫
    logging.getLogger('transformers').setLevel(logging.INFO)
    logging.getLogger('huggingface_hub').setLevel(logging.INFO)
    logging.getLogger('urllib3').setLevel(logging.INFO)

# –í—ã–∑–æ–≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–≥–µ—Ä–∞
setup_logger()
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ú–æ–¥–µ–ª–∏ Pydantic
class ReviewItem(BaseModel):
    text: str = Field(..., min_length=2)
    rating: float = Field(..., gt=0, le=5)
    date: Optional[str] = None

    @field_validator('text')
    @classmethod
    def validate_text(cls, v):
        return v.strip()

class BusinessReviewRequest(BaseModel):
    business_name: Optional[str] = "Unknown Place"
    extracted_reviews: List[ReviewItem]

    @field_validator('extracted_reviews')
    @classmethod
    def validate_reviews(cls, v):
        if not v:
            raise ValueError('At least one review is required')
        return v

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Business Reviews AI Analyzer",
    description="Advanced AI-powered review analysis service",
    version="1.0.0"
)

# Middleware –¥–ª—è CORS
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)


# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ML –º–æ–¥–µ–ª–∏
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
model = None
tokenizer = None

def initialize_model():
    global model, tokenizer
    try:
        logger.info(f"Attempting to initialize model: {MODEL_NAME}")

        # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Hugging Face (–µ—Å–ª–∏ –µ—Å—Ç—å —Ç–æ–∫–µ–Ω)
        if HF_TOKEN:
            logger.info("Logging in to Hugging Face")
            login(token=HF_TOKEN)
        else:
            logger.warning("No Hugging Face token provided")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        logger.info("Loading tokenizer")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

        # üö® –£–±–∏—Ä–∞–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é bitsandbytes!
        logger.info("Loading model without quantization (CPU Mode)")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float32 –¥–ª—è CPU
            device_map={"": "cpu"}  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–≤–∏–º CPU
        )

        logger.info("Mistral model initialized successfully")
        return True

    except Exception as e:
        logger.error(f"Model initialization error: {e}")
        logger.error(f"Detailed traceback: {traceback.format_exc()}")
        return False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
initialize_model()

def calculate_sentiment(reviews: List[ReviewItem]) -> float:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –±–∞–ª–ª–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        logger.debug("=" * 80)
        logger.debug("ENTERING calculate_sentiment")
        logger.debug(f"Total Reviews: {len(reviews)}")

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–∞
        for i, review in enumerate(reviews, 1):
            logger.debug(f"Review {i} Rating: {review.rating}")

        if not reviews:
            logger.warning("No reviews for averageScore calculation")
            return 0.0

        # –¢–æ—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –±–∞–ª–ª–∞
        ratings = [review.rating for review in reviews]
        avg_rating = sum(ratings) / len(ratings)

        # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ –æ–¥–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
        result = round(avg_rating, 1)

        logger.debug(f"Calculated Sentiment: {result}")
        logger.debug("=" * 80)
        return result
    except Exception as e:
        logger.error(f"Error calculating averageScore: {e}")
        return 0.0

def trim_to_full_sentence(text, max_length=500):
    """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ max_length, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ–ª–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
    if len(text) <= max_length:
        return text  # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏ —Ç–∞–∫ –∫–æ—Ä–æ—á–µ –ª–∏–º–∏—Ç–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å

    truncated_text = text[:max_length]  # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–µ–º –ø–æ –ª–∏–º–∏—Ç—É
    last_sentence_end = max(truncated_text.rfind('.'), truncated_text.rfind('!'), truncated_text.rfind('?'))

    if last_sentence_end == -1:  # –ï—Å–ª–∏ –Ω–µ—Ç –∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        return truncated_text

    return truncated_text[:last_sentence_end+1]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∑–Ω–∞–∫–∞


def generate_mistral_summary(reviews: list, business_name: str = "Unknown Place") -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫, –∑–∞–≤–µ—Ä—à–∞—è –µ–≥–æ –Ω–∞ –∑–Ω–∞–∫–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        if model is None:
            return "Model initialization failed"

        if tokenizer is None:
            return "Tokenizer initialization failed"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—Ç–∑—ã–≤–æ–≤
        if not reviews:
            return "No reviews available for analysis"

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
        reviews = [review[:500] for review in reviews]  # –ú–∞–∫—Å–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –æ—Ç–∑—ã–≤

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
        prompt = f"""<s>[INST] Summarize the following customer reviews about {business_name} in 3 sentences: 
Provide a comprehensive summary (500 tokens max) that includes:
1. Overall sentiment
2. Main positive points
3. Main criticisms

Reviews:
"""
        for review in reviews[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 –æ—Ç–∑—ã–≤–æ–≤
            prompt += f"- {review}\n"
        prompt += "[/INST]"

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048, padding=True).to(model.device)

        outputs = model.generate(
            inputs["input_ids"],
            max_new_tokens=400,
            attention_mask=inputs["attention_mask"],
            temperature=0.5,
            top_p=0.85,
            repetition_penalty=1.2,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∞–º–º–∞—Ä–∏
        if "[/INST]" in full_response:
            summary = full_response.split("[/INST]")[-1].strip()
        else:
            summary = full_response.strip()

        # –û–±—Ä–µ–∑–∞–µ–º, —Å–æ—Ö—Ä–∞–Ω—è—è –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        summary = trim_to_full_sentence(summary, 500)

        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø—É—Å—Ç–æ–µ —Å–∞–º–º–∞—Ä–∏, –¥–∞—ë–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç
        if not summary:
            summary = "Comprehensive analysis of customer reviews reveals mixed experiences. Specific details about service quality, customer satisfaction, and key characteristics could not be definitively extracted."

        return summary

    except Exception as e:
        return "Unable to process reviews and generate summary at this time."

def extract_themes(summary: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º"""
    try:
        theme_indicators = [
            "positive aspects", "negative aspects", "criticized for",
            "praised for", "customers appreciate", "complaints about",
            "notable features", "key issues"
        ]

        themes = []
        for indicator in theme_indicators:
            if indicator in summary.lower():
                match = re.search(rf'{indicator}[:\s]*([^.;,]+)', summary, re.IGNORECASE)
                if match:
                    theme = match.group(1).strip()
                    if theme and theme not in themes:
                        themes.append(theme.capitalize())

        # –¢–µ–º—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if not themes:
            themes = ["Service Quality", "Customer Experience", "Atmosphere"]

        logger.debug(f"Extracted Themes: {themes}")
        return themes[:4]
    except Exception as e:
        logger.error(f"Error extracting themes: {e}")
        return ["Service Quality", "Customer Experience", "Atmosphere"]

# –ù–û–í–´–ô –≠–ù–î–ü–û–ò–ù–¢: –ü—Ä–∏–Ω–∏–º–∞–µ—Ç JSON –Ω–∞–ø—Ä—è–º—É—é –≤–º–µ—Å—Ç–æ —Ñ–∞–π–ª–∞
@app.post("/analyze-business-reviews-json")
async def analyze_business_reviews_json(payload: Dict[str, Any] = Body(...)):
    try:
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –ª–æ–≥ –Ω–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        with open(os.path.join(LOG_DIR, "request_log.txt"), "a", encoding="utf-8") as f:
            f.write(f"\n\n=== NEW REQUEST {datetime.datetime.now()} ===\n")
            f.write(f"Payload keys: {list(payload.keys())}\n")

        logger.debug("=" * 100)
        logger.debug("ENTERING analyze_business_reviews_json ENDPOINT")
        logger.debug(f"Received payload with keys: {list(payload.keys())}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–∞ —Å –æ—Ç–∑—ã–≤–∞–º–∏
        reviews_keys = [
            'Extracted Reviews',
            'extracted_reviews',
            'reviews',
            'Extracted_Reviews'
        ]

        reviews_key = next((key for key in reviews_keys if key in payload), None)

        if not reviews_key:
            logger.error("No reviews key found")
            logger.error(f"Available keys: {list(payload.keys())}")
            return JSONResponse(
                status_code=422,
                content={
                    "error": "No reviews found",
                    "details": f"Expected one of {reviews_keys}",
                    "available_keys": list(payload.keys())
                }
            )

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –±–∏–∑–Ω–µ—Å–∞
        business_name = payload.get('Business Name', payload.get('business_name', 'Unknown Place'))
        logger.debug(f"Business Name: {business_name}")

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–ª—é—á–∞
        logger.debug(f"Found reviews key: {reviews_key}")
        logger.debug(f"Number of reviews: {len(payload[reviews_key])}")

        # –ì–∏–±–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤
        extracted_reviews = []
        for item in payload[reviews_key]:
            try:
                review_text = str(item.get('text', '')).strip()
                review_rating = float(item.get('rating', 0))
                review_date = item.get('date')

                if review_text and review_rating:
                    extracted_reviews.append(
                        ReviewItem(
                            text=review_text,
                            rating=review_rating,
                            date=review_date
                        )
                    )
            except Exception as review_error:
                logger.warning(f"Skipping problematic review: {review_error}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—Ç–∑—ã–≤–æ–≤
        if not extracted_reviews:
            logger.error("No valid reviews could be extracted")
            return JSONResponse(
                status_code=422,
                content={
                    "error": "No valid reviews",
                    "details": "No valid reviews could be extracted from the input"
                }
            )

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤
        review_texts = [review.text for review in extracted_reviews]

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞
        summary = generate_mistral_summary(review_texts, business_name)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º
        key_themes = extract_themes(summary)

        # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –±–∞–ª–ª–∞
        average_score = calculate_sentiment(extracted_reviews)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç
        dates = [review.date for review in extracted_reviews if review.date]
        first_date = min(dates) if dates else None
        last_date = max(dates) if dates else None

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞
        result = {
            "nameOfPlace": business_name,
            "summary": summary,
            "averageScore": average_score,
            "keyThemes": key_themes,
            "firstReviewDate": first_date,
            "lastReviewDate": last_date
        }

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—Ç
        logger.debug(f"Final result: {json.dumps(result, indent=2)}")
        logger.debug("=" * 100)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        with open(os.path.join(LOG_DIR, "request_log.txt"), "a", encoding="utf-8") as f:
            f.write(f"Result: {json.dumps(result, indent=2)}\n")

        return JSONResponse(
            content=result,
            media_type="application/json; charset=utf-8"
        )

    except Exception as e:
        logger.error(f"CRITICAL ENDPOINT ERROR: {e}", exc_info=True)
        # –ó–∞–ø–∏—Å—å –æ—à–∏–±–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥
        with open(os.path.join(LOG_DIR, "error_log.txt"), "a", encoding="utf-8") as f:
            f.write(f"\n\n=== ERROR {datetime.datetime.now()} ===\n")
            f.write(f"Error: {str(e)}\n")
            f.write(traceback.format_exc())

        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal server error",
                "details": str(e)
            },
            media_type="application/json; charset=utf-8"
        )

@app.get("/health")
async def health_check():
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏"""
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Å—Ç–∞—Ç—É—Å–∞
        status = "healthy" if model is not None and tokenizer is not None else "unhealthy"

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
        logger.info(f"Health check - Status: {status}")

        return JSONResponse(
            content={
                "status": status,
                "model": MODEL_NAME,
                "message": "Review analysis service is operational",
                "details": {
                    "model_loaded": model is not None,
                    "tokenizer_loaded": tokenizer is not None
                }
            },
            media_type="application/json; charset=utf-8"
        )
    except Exception as e:
        logger.error(f"Error during health check: {e}")
        return JSONResponse(
            content={
                "status": "error",
                "message": str(e)
            },
            status_code=500,
            media_type="application/json; charset=utf-8"
        )

# –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
@app.post("/echo")
async def echo(data: Dict[str, Any] = Body(...)):
    return JSONResponse(
        content={
            "received": data,
            "message": "This is an echo endpoint for debugging"
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)